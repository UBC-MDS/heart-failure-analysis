{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: Heart Failure Survival Analysis\n",
        "author: \"Merari Santana, Kevin Gao, Gurmehak Kaur, Yuhan Fan\"\n",
        "jupyter: python3\n",
        "format:\n",
        "  html:\n",
        "    toc: true\n",
        "    toc-depth: 2\n",
        "    self-contained: true\n",
        "bibliography: references.bib\n",
        "execute:\n",
        "    echo: false\n",
        "---"
      ],
      "id": "b11e29fe"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "#| output: false\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import altair as alt\n",
        "import altair_ally as aly\n",
        "import os\n",
        "from vega_datasets import data\n",
        "from sklearn import set_config\n",
        "from sklearn.model_selection import (GridSearchCV, cross_validate, train_test_split,)\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import make_column_transformer\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
        "import pandera as pa\n",
        "from deepchecks.tabular.checks import FeatureLabelCorrelation, FeatureFeatureCorrelation\n",
        "from deepchecks.tabular import Dataset\n",
        "import warnings\n",
        "\n",
        "\n",
        "# Enable Vegafusion for better data transformation\n",
        "#aly.alt.data_transformers.enable('vegafusion')\n",
        "#alt.data_transformers.enable('vegafusion')"
      ],
      "id": "7e05688f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "#| output: false\n",
        "#| label: tbl-model-metrics\n",
        "#| tbl-cap: Evaluation metrics for the final model.\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "file_path = '../data/heart_failure_clinical_records_dataset.csv'\n",
        "heart_failure_data = pd.read_csv(file_path)\n",
        "\n",
        "# List of binary columns\n",
        "binary_columns = ['anaemia', 'diabetes', 'high_blood_pressure', 'sex', 'smoking', 'DEATH_EVENT']\n",
        "\n",
        "# Convert all binary columns to True/False\n",
        "heart_failure_data[binary_columns] = heart_failure_data[binary_columns].astype(bool)\n",
        "\n",
        "heart_failure_data.shape\n",
        "\n",
        "heart_failure_data.info()\n",
        "\n",
        "heart_failure_data['DEATH_EVENT'].value_counts()\n",
        "\n",
        "# Summary statistics\n",
        "print(\"Summary Statistics:\")\n",
        "heart_failure_data.describe()\n",
        "\n",
        "# Check for missing values\n",
        "missing_values = heart_failure_data.isnull().sum()\n",
        "\n",
        "# Convert to a DataFrame for better visualization\n",
        "missing_values_df = pd.DataFrame({\n",
        "    'Column': missing_values.index,\n",
        "    'Missing Values': missing_values.values\n",
        "})\n",
        "\n",
        "# Display the DataFrame\n",
        "missing_values_df\n",
        "\n",
        "aly.heatmap(heart_failure_data,color=\"DEATH_EVENT\")\n",
        "\n",
        "\n",
        "# Distributions of all columns\n",
        "print(\"Visualizing distributions for all columns...\")\n",
        "aly.dist(heart_failure_data)\n",
        "\n",
        "\n",
        "aly.pair(heart_failure_data,color=\"DEATH_EVENT\")\n",
        "\n",
        "aly.corr(data.movies())\n",
        "\n",
        "aly.parcoord(heart_failure_data,color = 'DEATH_EVENT')\n",
        "\n",
        "# Create the distribution plots\n",
        "aly.dist(heart_failure_data,color = 'DEATH_EVENT')\n",
        "\n",
        "#Data Splitting!\n",
        "\n",
        "heart_failure_data = pd.read_csv(file_path)\n",
        "\n",
        "heart_failure_train, heart_failure_test = train_test_split(heart_failure_data, \n",
        "                                                           train_size = 0.8, \n",
        "                                                           stratify = heart_failure_data['DEATH_EVENT'],\n",
        "                                                           random_state = 522)\n",
        "\n",
        "url_processed = '../data/processed/'\n",
        "heart_failure_train.to_csv(os.path.join(url_processed, 'heart_failure_train.csv'))\n",
        "heart_failure_test.to_csv(os.path.join(url_processed, 'heart_failure_test.csv'))\n",
        "\n",
        "#Preprocessing Columns!\n",
        "\n",
        "# Define numeric columns\n",
        "numeric_columns = ['age', 'creatinine_phosphokinase', 'ejection_fraction', \n",
        "                   'platelets', 'serum_creatinine', 'serum_sodium', 'time']\n",
        "# List of binary columns\n",
        "binary_columns = ['anaemia', 'diabetes', 'high_blood_pressure', 'sex', 'smoking']\n",
        "\n",
        "# Convert all binary columns to True/False so they're treated as categorical data\n",
        "heart_failure_train[binary_columns] = heart_failure_train[binary_columns].astype(bool)\n",
        "heart_failure_test[binary_columns] = heart_failure_test[binary_columns].astype(bool)\n",
        "\n",
        "\n",
        "\n",
        "preprocessor = make_column_transformer(\n",
        "    (StandardScaler(), numeric_columns),\n",
        "    (OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False, drop='if_binary', dtype = int), binary_columns),\n",
        "    remainder = 'passthrough'\n",
        ")\n",
        "\n",
        "preprocessor.fit(heart_failure_train)\n",
        "heart_failure_scaled_train = preprocessor.transform(heart_failure_train)\n",
        "heart_failure_scaled_test = preprocessor.transform(heart_failure_test)\n",
        "\n",
        "preprocessor.verbose_feature_names_out = False\n",
        "column_names = (preprocessor.get_feature_names_out().tolist())\n",
        "scaled_train = pd.DataFrame(heart_failure_scaled_train, columns=column_names)\n",
        "\n",
        "#Correlation matrix part starts from here:\n",
        "correlation_matrix = scaled_train.drop(columns=['DEATH_EVENT']).corr()\n",
        "correlation_long = correlation_matrix.reset_index().melt(id_vars='index')\n",
        "correlation_long.columns = ['Feature 1', 'Feature 2', 'Correlation']\n",
        "\n",
        "correlation_heatmap = alt.Chart(correlation_long).mark_rect().encode(\n",
        "    x='Feature 1:O',\n",
        "    y='Feature 2:O',\n",
        "    color=alt.Color('Correlation:Q', scale=alt.Scale(scheme='viridis')),\n",
        "    tooltip=['Feature 1', 'Feature 2', 'Correlation']\n",
        ").properties(\n",
        "    width=600,\n",
        "    height=600,\n",
        "    title=\"Correlation Heatmap\"\n",
        ")\n",
        "\n",
        "\n",
        "# Specify the folder path and save the chart\n",
        "folder_path = '../results/figures'  \n",
        "correlation_heatmap.save(f'{folder_path}/correlation_heatmap.png')\n",
        "\n",
        "\n",
        "correlation_heatmap\n",
        "\n",
        "#Based on the correlation matrix graph below, all features have relatively low correlations between each other, \n",
        "#the correlations are below 0.5, so there is no strong evidence to drop additional featues. \n",
        "\n",
        "# validate training data for anomalous correlations between target/response variable \n",
        "# and features/explanatory variables, \n",
        "# as well as anomalous correlations between features/explanatory variables\n",
        "# Do these on training data as part of EDA! \n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"deepchecks\")\n",
        "\n",
        "scaled_train_ds = Dataset(scaled_train, label=\"DEATH_EVENT\", cat_features=[])\n",
        "\n",
        "check_feat_lab_corr = FeatureLabelCorrelation().add_condition_feature_pps_less_than(0.9)\n",
        "check_feat_lab_corr_result = check_feat_lab_corr.run(dataset=scaled_train_ds)\n",
        "\n",
        "check_feat_feat_corr = FeatureFeatureCorrelation().add_condition_max_number_of_pairs_above_threshold(threshold = 0.92, n_pairs = 0)\n",
        "check_feat_feat_corr_result = check_feat_feat_corr.run(dataset=scaled_train_ds)\n",
        "\n",
        "if not check_feat_lab_corr_result.passed_conditions():\n",
        "    raise ValueError(\"Feature-Label correlation exceeds the maximum acceptable threshold.\")\n",
        "\n",
        "if not check_feat_feat_corr_result.passed_conditions():\n",
        "    raise ValueError(\"Feature-feature correlation exceeds the maximum acceptable threshold.\")\n",
        "\n",
        "#Building the model!\n",
        "\n",
        "#Decision Tree\n",
        "pipeline = make_pipeline(\n",
        "        preprocessor, \n",
        "        DecisionTreeClassifier(random_state=522)\n",
        "    )\n",
        "\n",
        "dt_scores = cross_validate(pipeline, \n",
        "                           heart_failure_train.drop(columns=['DEATH_EVENT']), \n",
        "                           heart_failure_train['DEATH_EVENT'],\n",
        "                           return_train_score=True\n",
        "                          )\n",
        "\n",
        "dt_scores = pd.DataFrame(dt_scores).sort_values('test_score', ascending = False)\n",
        "dt_scores\n",
        "\n",
        "\n",
        "#KNN\n",
        "pipeline = make_pipeline(\n",
        "        preprocessor, \n",
        "        KNeighborsClassifier()\n",
        "    )\n",
        "\n",
        "param_grid = {\n",
        "    \"kneighborsclassifier__n_neighbors\": range(1, 100, 3)\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(\n",
        "    pipeline,\n",
        "    param_grid,\n",
        "    cv=10,  \n",
        "    n_jobs=-1,  \n",
        "    return_train_score=True,\n",
        ")\n",
        "\n",
        "heart_failure_fit = grid_search.fit(heart_failure_train.drop(columns=['DEATH_EVENT']), heart_failure_train['DEATH_EVENT'] )\n",
        "\n",
        "knn_best_model = grid_search.best_estimator_ \n",
        "knn_best_model\n",
        "\n",
        "pd.DataFrame(grid_search.cv_results_).sort_values('mean_test_score', ascending = False)[['params', 'mean_test_score']].iloc[0]\n",
        "\n",
        "#Logistic Regression\n",
        "\n",
        "pipeline = make_pipeline(\n",
        "        preprocessor, \n",
        "        LogisticRegression(random_state=522, max_iter=2000, class_weight = \"balanced\")\n",
        "    )\n",
        "\n",
        "param_grid = {\n",
        "    \"logisticregression__C\": 10.0 ** np.arange(-5, 5, 1)\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(\n",
        "    pipeline,\n",
        "    param_grid,\n",
        "    cv=10,  \n",
        "    n_jobs=-1,  \n",
        "    return_train_score=True\n",
        ")\n",
        "\n",
        "heart_failure_fit = grid_search.fit(heart_failure_train.drop(columns=['DEATH_EVENT']), heart_failure_train['DEATH_EVENT'] )\n",
        "\n",
        "lr_best_model = grid_search.best_estimator_.named_steps['logisticregression']\n",
        "lr_best_model\n",
        "\n",
        "lr_scores = pd.DataFrame(grid_search.cv_results_).sort_values('mean_test_score', ascending = False)[['param_logisticregression__C', 'mean_test_score', 'mean_train_score']]\n",
        "lr_scores.iloc[0:5]\n",
        "\n",
        "# Log scale for x-axis, fixed y-axis range, and explicit data type specification\n",
        "lr_train_test_cv = alt.Chart(lr_scores).transform_fold(\n",
        "    [\"mean_test_score\", \"mean_train_score\"],  # Combine columns into one for color differentiation\n",
        "    as_=[\"Score Type\", \"Score\"]  # Rename columns for legend and y-axis\n",
        ").mark_line().encode(\n",
        "    x=alt.X(\"param_logisticregression__C:Q\", \n",
        "            title=\"C (Regularization Parameter)\", \n",
        "            scale=alt.Scale(type='log')),  # Set x-axis to log scale\n",
        "    y=alt.Y(\"Score:Q\", \n",
        "            title=\"Score\", \n",
        "            scale=alt.Scale(domain=[0.75, 0.85])),  # Set y-axis range\n",
        "    color=alt.Color(\"Score Type:N\", \n",
        "                    title=\"Score Type\",  # Add legend title\n",
        "                    scale=alt.Scale(domain=[\"mean_test_score\", \"mean_train_score\"],\n",
        "                                    range=[\"skyblue\", \"pink\"])),  # Map colors to lines\n",
        "    tooltip=[\"param_logisticregression__C\", \"Score Type:N\", \"Score:Q\"]  # Explicitly specify data types in tooltip\n",
        ").properties(\n",
        "    title=\"Training vs Cross-Validation Scores (Log Scale)\",\n",
        "    width=600,\n",
        "    height=400\n",
        ")\n",
        "\n",
        "# Specify the folder path and save the chart\n",
        "folder_path = '../results/figures'  \n",
        "lr_train_test_cv.save(f'{folder_path}/lr_cv_scores.png')\n",
        "\n",
        "lr_train_test_cv\n",
        "\n",
        "features = lr_best_model.coef_\n",
        "feature_names = heart_failure_train.drop(columns=['DEATH_EVENT']).columns\n",
        "coefficients = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Coefficient': features[0],\n",
        "    'Absolute_Coefficient': abs(features[0])\n",
        "}).sort_values(by='Absolute_Coefficient', ascending=False)\n",
        "\n",
        "coefficients\n",
        "\n",
        "#Model Evaluation!\n",
        "\n",
        "#Confusion Matrix\n",
        "\n",
        "# Confusion Matrix\n",
        "\n",
        "heart_failure_predictions = heart_failure_test.assign(\n",
        "    predicted=heart_failure_fit.predict(heart_failure_test)\n",
        ")\n",
        "\n",
        "cm_crosstab = pd.crosstab(heart_failure_predictions['DEATH_EVENT'], \n",
        "                          heart_failure_predictions['predicted'], \n",
        "                          rownames=[\"Actual\"], \n",
        "                          colnames=[\"Predicted\"]\n",
        "                         )\n",
        "\n",
        "\n",
        "cm_crosstab\n",
        "# cm = confusion_matrix(heart_failure_test[\"DEATH_EVENT\"], heart_failure_fit.predict(heart_failure_test))\n",
        "# cm\n",
        "\n",
        "accuracy = round(accuracy_score(heart_failure_predictions['DEATH_EVENT'], heart_failure_predictions['predicted']),2)\n",
        "precision = round(precision_score(heart_failure_predictions['DEATH_EVENT'], heart_failure_predictions['predicted']),2)\n",
        "recall = round(recall_score(heart_failure_predictions['DEATH_EVENT'], heart_failure_predictions['predicted']),2)\n",
        "f1 = round(f1_score(heart_failure_predictions['DEATH_EVENT'], heart_failure_predictions['predicted']),2)\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")\n"
      ],
      "id": "tbl-model-metrics",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: tbl-confusion-matrix\n",
        "#| tbl-cap: Confusion matrix for the final model on the test dataset.\n",
        "#| echo: false\n",
        "#| output: false\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Generate confusion matrix as a crosstab\n",
        "heart_failure_predictions2 = heart_failure_test.assign(\n",
        "    predicted=heart_failure_fit.predict(heart_failure_test)\n",
        ")\n",
        "\n",
        "cm_crosstab = pd.crosstab(\n",
        "    heart_failure_predictions['DEATH_EVENT'], \n",
        "    heart_failure_predictions['predicted'], \n",
        "    rownames=[\"Actual\"], \n",
        "    colnames=[\"Predicted\"]\n",
        ")\n",
        "TP = cm_crosstab.iloc[1,1]\n",
        "TN = cm_crosstab.iloc[0,0]\n",
        "FP = cm_crosstab.iloc[0,1]\n",
        "FN = cm_crosstab.iloc[1,0]\n",
        "\n",
        "# Render the confusion matrix as it is for correct alignment\n",
        "cm_crosstab.style.set_table_attributes(\"style='display:inline'\")"
      ],
      "id": "tbl-confusion-matrix",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "We built a classification model using the logistic regression algorithm to predict survival outcomes for patients with heart failure. Using patient test results, the final classifier achieves an accuracy of `{python} accuracy`. The model’s precision of `{python} precision` suggests it is moderately conservative in predicting the positive class (death), minimizing false alarms. More importantly, the recall of `{python} recall` ensures the model identifies the majority of high-risk patients, reducing the likelihood of missing true positive cases, which could have serious consequences. The F1-score of `{python} f1` reflects a good balance between precision and recall, highlighting the model’s robustness in survival prediction, see @tbl-model-metrics2.\n",
        "\n",
        "From the confusion matrix, the model correctly identified `{python} TP` patients who passed away (true positives) and`{python} TN` patients who survived (true negatives). However, it also predicted `{python} FP` false positives, incorrectly classifying some survivors as deceased, and missed `{python} FN` actual cases of death (false negatives). While these errors warrant consideration, the model’s performance demonstrates strong predictive capabilities for both positive and negative outcomes, see @tbl-confusion-matrix2.\n",
        "\n",
        "Overall, the logistic regression classifier effectively leverages patient test results to support survival prediction, providing a valuable tool to aid clinical decision-making in heart failure management.\n",
        "\n",
        "## Introduction\n",
        "\n",
        "Cardiovascular diseases are responsible for approximately 17 million deaths globally each year, with heart failure and myocardial infarctions being the leading contributors to this staggering toll [@chicco2020]. Electronic medical records from patients with heart failure, collected during follow-up care, provide a wealth of data on symptoms, test results, and clinical outcomes. Leveraging this data, our team applies machine learning algorithms to predict patient survival after heart failure. This approach uncovers critical patterns and insights that might otherwise remain hidden from traditional clinical assessments, offering valuable tools to support medical decision-making and improve patient outcomes. \n",
        "\n",
        "## Data \n",
        "\n",
        "We analyzed a dataset containing the medical records of 299 heart failure patients [@dua2017]. The patients consisted of 105 women and 194 men, and their ages range between 40 and 95 years old. The dataset contains 13 features shown in @tbl-patient-table, which report clinical, body, and lifestyle information [@heartfailuredata]. The **death event** was used as the target variable in our binary classification study. It states whether the patient died or survived before the end of the follow-up period, which lasted 130 days on average. Our dataset has a class imbalance where the number of survived patients (death event = 0) is 203 (67.89%) and the number of dead patients (death event = 1) is 96 (32.11%), see @tbl-death-event-counts.\n",
        "\n",
        "\n",
        "\n",
        "## Explanatory Data Analysis and Visualizations\n"
      ],
      "id": "cde8bb4c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: tbl-patient-table\n",
        "#| tbl-cap: Description of the columns in the heart failure dataset.\n",
        "#| echo: false\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Load the table\n",
        "patient_table = pd.read_csv(\"../results/tables/patient_table.csv\")\n",
        "\n",
        "# Apply CSS to left-align all columns\n",
        "patient_table.style.set_properties(**{'text-align': 'left'})"
      ],
      "id": "tbl-patient-table",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![Correlation heatmap-  Based on the correlation matrix graph below, all features have relatively low correlations between each other, the correlations are below 0.5, so there is no strong evidence to drop additional featues. ](../results/figures/correlation_heatmap.png){#correlation_heatmap width=60% fig-pos=\"H\"}\n"
      ],
      "id": "8f903165"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: tbl-missing-values\n",
        "#| tbl-cap: Number of missing values in each column of the heart failure dataset.\n",
        "#| echo: false\n",
        "\n",
        "file_path = '../data/heart_failure_clinical_records_dataset.csv'\n",
        "heart_failure_data = pd.read_csv(file_path)\n",
        "\n",
        "# Check for missing values\n",
        "missing_values = heart_failure_data.isnull().sum()\n",
        "\n",
        "# Convert to a DataFrame for better visualization\n",
        "missing_values_df = pd.DataFrame({\n",
        "    'Column': missing_values.index,\n",
        "    'Missing Values': missing_values.values\n",
        "})\n",
        "\n",
        "# Style the DataFrame to align text to the left\n",
        "missing_values_df.style.set_properties(**{'text-align': 'left'})"
      ],
      "id": "tbl-missing-values",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "No missing values, no imputation or filling Nulls required\n"
      ],
      "id": "580a8175"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: tbl-death-event-counts\n",
        "#| tbl-cap: Distribution of the target variable `DEATH_EVENT` in the heart failure dataset.\n",
        "#| echo: false\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "file_path = '../data/heart_failure_clinical_records_dataset.csv'\n",
        "heart_failure_data = pd.read_csv(file_path)\n",
        "\n",
        "# Get value counts for DEATH_EVENT and convert to a DataFrame\n",
        "death_event_counts = heart_failure_data['DEATH_EVENT'].value_counts().reset_index()\n",
        "death_event_counts.columns = ['DEATH_EVENT', 'Count']  # Rename columns\n",
        "\n",
        "# Display the DataFrame as a table\n",
        "death_event_counts"
      ],
      "id": "tbl-death-event-counts",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Dataset Size: The dataset is relatively small, with only 300 rows. \\\n",
        "Class Imbalance: The target variable, DEATH_EVENT, has few examples in the \"True\" class (i.e., the event occurred), which might affect the model's ability to learn and generalize well. This class imbalance will be taken into consideration during analysis and model evaluation.\n",
        "\n",
        "\n",
        "\n",
        "## Model\n",
        "\n",
        "We compared Decision Tree, KNN, Logistic Regression, and selected Logistic Regression due to its interpretability, and ability to handle both linear and non-linear relationships between features. Logistic Regression performed better than the other two models as it works well with fewer features and is less prone to overfitting compared to more complex models like Decision Trees or KNN, especially when the data is relatively small.\n"
      ],
      "id": "98391a6e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "heart_failure_data = pd.read_csv(file_path)\n",
        "\n",
        "heart_failure_train, heart_failure_test = train_test_split(heart_failure_data, \n",
        "                                                           train_size = 0.8, \n",
        "                                                           stratify = heart_failure_data['DEATH_EVENT'],\n",
        "                                                           random_state = 522)\n",
        "\n",
        "url_processed = '../data/processed/'\n",
        "heart_failure_train.to_csv(os.path.join(url_processed, 'heart_failure_train.csv'))\n",
        "heart_failure_test.to_csv(os.path.join(url_processed, 'heart_failure_test.csv'))\n",
        "\n",
        "# Define numeric columns\n",
        "numeric_columns = ['age', 'creatinine_phosphokinase', 'ejection_fraction', \n",
        "                   'platelets', 'serum_creatinine', 'serum_sodium', 'time']\n",
        "# List of binary columns\n",
        "binary_columns = ['anaemia', 'diabetes', 'high_blood_pressure', 'sex', 'smoking']\n",
        "\n",
        "# Convert all binary columns to True/False so they're treated as categorical data\n",
        "heart_failure_train[binary_columns] = heart_failure_train[binary_columns].astype(bool)\n",
        "heart_failure_test[binary_columns] = heart_failure_test[binary_columns].astype(bool)\n",
        "\n",
        "preprocessor = make_column_transformer(\n",
        "    (StandardScaler(), numeric_columns),\n",
        "    (OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False, drop='if_binary', dtype = int), binary_columns),\n",
        "    remainder = 'passthrough'\n",
        ")"
      ],
      "id": "19fb9ca5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "#| output: false\n",
        "\n",
        "pipeline = make_pipeline(\n",
        "        preprocessor, \n",
        "        LogisticRegression(random_state=522, max_iter=2000, class_weight = \"balanced\")\n",
        "    )\n",
        "\n",
        "param_grid = {\n",
        "    \"logisticregression__C\": 10.0 ** np.arange(-5, 5, 1)\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(\n",
        "    pipeline,\n",
        "    param_grid,\n",
        "    cv=10,  \n",
        "    n_jobs=-1,  \n",
        "    return_train_score=True\n",
        ")\n",
        "\n",
        "heart_failure_fit = grid_search.fit(heart_failure_train.drop(columns=['DEATH_EVENT']), heart_failure_train['DEATH_EVENT'] )\n",
        "\n",
        "lr_best_model = grid_search.best_estimator_.named_steps['logisticregression']\n",
        "lr_best_model"
      ],
      "id": "c4876708",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Hyperparameter tuning to find find the best Logistic Regression model:"
      ],
      "id": "b90a7b6f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "lr_scores = pd.DataFrame(grid_search.cv_results_).sort_values('mean_test_score', ascending = False)[['param_logisticregression__C', 'mean_test_score', 'mean_train_score']]\n",
        "logregC = lr_scores.iloc[0,0]\n",
        "logreg_cv = round(lr_scores.iloc[0,1],2)\n",
        "lr_scores.iloc[0:5]"
      ],
      "id": "e2de2439",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**The model is performing well with C = `{python} logregC` with a CV score of `{python} logreg_cv` and is close to train score, indicating that model is generalising well.**\n",
        "\n",
        "![Cross-validation scores for Logistic Regression -  Logistic regression performs better than Decision tree and KNN on the cross validation data, hence, we selected it as our final model.](../results/figures/lr_cv_scores.png){#fig-lr_cv_scores width=60% fig-pos=\"H\"}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "The best features to train our model are show in @tbl-top-features:\n"
      ],
      "id": "af791c31"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: tbl-top-features\n",
        "#| tbl-cap: Top features for trainig the model.\n",
        "#| echo: false\n",
        "\n",
        "features = lr_best_model.coef_\n",
        "feature_names = heart_failure_train.drop(columns=['DEATH_EVENT']).columns\n",
        "coefficients = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Coefficient': features[0],\n",
        "    'Absolute_Coefficient': abs(features[0])\n",
        "}).sort_values(by='Absolute_Coefficient', ascending=False)\n",
        "\n",
        "coefficients"
      ],
      "id": "tbl-top-features",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Evaluation\n",
        "\n",
        "#### Confusion Matrix\n"
      ],
      "id": "badd4f0f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: tbl-confusion-matrix2\n",
        "#| tbl-cap: Confusion matrix for the final model on the test dataset.\n",
        "#| echo: false\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Generate confusion matrix as a crosstab\n",
        "heart_failure_predictions = heart_failure_test.assign(\n",
        "    predicted=heart_failure_fit.predict(heart_failure_test)\n",
        ")\n",
        "\n",
        "cm_crosstab2 = pd.crosstab(\n",
        "    heart_failure_predictions['DEATH_EVENT'], \n",
        "    heart_failure_predictions['predicted'], \n",
        "    rownames=[\"Actual\"], \n",
        "    colnames=[\"Predicted\"]\n",
        ")\n",
        "TP2 = cm_crosstab.iloc[1,1]\n",
        "TN2 = cm_crosstab.iloc[0,0]\n",
        "FP2 = cm_crosstab.iloc[0,1]\n",
        "FN2 = cm_crosstab.iloc[1,0]\n",
        "\n",
        "# Render the confusion matrix as it is for correct alignment\n",
        "cm_crosstab.style.set_table_attributes(\"style='display:inline'\")"
      ],
      "id": "tbl-confusion-matrix2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "#| label: tbl-model-metrics2\n",
        "#| tbl-cap: Evaluation metrics for the final model.\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "accuracy2 = accuracy_score(heart_failure_predictions['DEATH_EVENT'], heart_failure_predictions['predicted'])\n",
        "precision2 = precision_score(heart_failure_predictions['DEATH_EVENT'], heart_failure_predictions['predicted'])\n",
        "recall2 = recall_score(heart_failure_predictions['DEATH_EVENT'], heart_failure_predictions['predicted'])\n",
        "f1_2 = f1_score(heart_failure_predictions['DEATH_EVENT'], heart_failure_predictions['predicted'])\n",
        "\n",
        "# Create a DataFrame for the metrics\n",
        "metrics_table2 = pd.DataFrame({\n",
        "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1 Score'],\n",
        "    'Value': [accuracy, precision, recall, f1]\n",
        "})\n",
        "\n",
        "# Round values for better readability\n",
        "metrics_table2['Value'] = metrics_table2['Value'].round(4)\n",
        "\n",
        "# Display the DataFrame\n",
        "metrics_table2"
      ],
      "id": "tbl-model-metrics2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Results and Conculsion\n",
        "\n",
        "The analysis revealed that `platelets` and `ejection_fraction` are the most important features (see @tbl-top-features) in predicting the risk of patient mortality. These features significantly impact the model's ability to assess patient risk, which is crucial for early intervention. Our model achieved a recall score of `{python} recall` (see @tbl-model-metrics2), which is a good start, but there is room for improvement, particularly in reducing the number of high risk patients the model might miss, i.e., maximising recall by minimising False Negatives.\n",
        "\n",
        "The main challenges in this project stem from class imbalance and limited data availability. With more diverse and comprehensive datasets, performance could be further enhanced. We would also like to explore other machine learning models to improve the overall accuracy.\n",
        "\n",
        "In conclusion, while the current model shows potential, there is significant opportunity to enhance its effectiveness. With improvements in data quality and model optimization, this tool could become a crucial asset in predicting patient risk and saving lives.\n",
        "\n",
        "\n",
        "## References\n"
      ],
      "id": "4f5d6c21"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "C:\\Users\\Kevin\\miniforge3\\envs\\522\\share\\jupyter\\kernels\\python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}